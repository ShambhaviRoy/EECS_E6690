lm.less_noisy = lm(y1~x)
confint(lm.less_noisy)
y2 = -1 + .5*x + eps2
lm.more_noisy = lm(y2~x)
confint(lm.more_noisy)
?confint
eps1 = rnorm(100,sd = 0.15)
y1 = -1 + .5*x + eps1
lm.less_noisy = lm(y1~x)
confint(lm.less_noisy)
eps1 = rnorm(100,sd = 0.15)
y1 = -1 + .5*x + eps1
lm.less_noisy = lm(y1~x)
confint(lm.less_noisy)
confint(lm.less_noisy)
lm.less_noisy = lm(y1~x)
confint(lm.less_noisy)
eps1 = rnorm(100,sd = 0.15)
y1 = -1 + .5*x + eps1
lm.less_noisy = lm(y1~x)
confint(lm.less_noisy)
x = seq(-20,20)
clear
cls
clc
cl
clr
clear all
x = seq(-20,20)
plot(x,(1+3*x^2-2*x))
plot(x,(1+3*x^2-2*x),type='l')
abline(v=1/3)
abline(h=1/3)
x = seq(-5,5)
plot(x,(1+3*x^2-2*x),type='l')
abline(h=1/3)
x = seq(-3,3,0.01)
plot(x,(1+3*x^2-2*x),type='l')
abline(h=1/3)
x = seq(-0.33,0.33,0.01)
plot(x,(1+3*x^2-2*x),type='l')
abline(h=1/3)
x = seq(-10,10)
plot(x,(1+3*x^2-2*x),type='l')
abline(h=1/3)
plot(x,abs(x),type='l')
plot(x,((1-x)^2 + 2*abs(x)),type='l')
abline(h=0)
x
x = seq(-100,100)
plot(x,((1-x)^2 + 2*abs(x)),type='l')
abline(h=0)
x = seq(-20,20)
plot(x,((1-x)^2 + 2*abs(x)),type='l')
abline(h=0)
plot(x,((2-x)^2 + 2*abs(x)),type='l')
abline(h=1)
plot(x,((20-x)^2 + 10*abs(x)),type='l')
abline(h=5)
abline(h=10)
plot(x,((10-x)^2 + 5*abs(x)),type='l')
x = seq(-20,20)
plot(x,((10-x)^2 + 5*abs(x)),type='l')
abline(h=7.5)
plot(x,((10-x)^2 + 10*abs(x)),type='l')
abline(h=10)
abline(h=100)
abline(h=15)
x = seq(-200,200)
plot(x,((10-x)^2 + 10*abs(x)),type='l')
abline(h=10)
plot(x,((10-x)^2 + 10*abs(x)),type='l',xlim = c(0,100))
plot(x,((10-x)^2 + 10*abs(x)),type='l',ylim = c(0,100))
abline(h=10)
x = seq(-200,200,10)
plot(x,((10-x)^2 + 10*abs(x)),type='l',ylim = c(0,100))
x = seq(-5,5)
plot(x,(1+3*x^2-2*x),type='l')
plot(x,(1+3*x^2-2*x),type='l',xlim=c(-4,4))
abline(v=1/3)
x = seq(-5,5,0.01)
plot(x,(1+3*x^2-2*x),type='l',xlim=c(-4,4))
abline(v=1/3)
plot(x,((2-x)^2 + 2*abs(x)),type='l')
x = seq(-10,10,0.01)
plot(x,((2-x)^2 + 2*abs(x)),type='l')
plot(x,((2-x)^2 + 2*abs(x)),type='l',xlim = c(-2,2))
plot(x,((2-x)^2 + 2*abs(x)),type='l',xlim = c(-20,20))
plot(x,((2-x)^2 + 2*abs(x)),type='l',xlim = c(-10,10))
abline(v=1)
abline(h=3)
plot(x,((2-x)^2 + 2*abs(x)),type='l',xlim = c(-10,10))
abline(v=1)
X = rnorm(100)
eps = rnorm(100)
Y = 3 + 2 * X + 4 * X^2 + 2 * X^3 + eps
df = data.frame(y = Y, x = X)
bestss = regsubsets(y ~ poly(x, 10, raw = T), data = df, nvmax = 10)
library(leaps)
install.packages("leaps")
library(leaps)
df = data.frame(y = Y, x = X)
bestss = regsubsets(y ~ poly(x, 10, raw = T), data = df, nvmax = 10)
which.min(bestss.summary$cp)
bestss.summary = summary(bestss)
which.min(bestss.summary$cp)
which.min(bestss.summary$bic)
which.min(bestss.summary$adjr2)
plot(bestss.summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(3, bestss.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
which.min(bestss.summary$adjr2)
which.min(bestss.summary$bic)
plot(bestss.summary$cp, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(3, bestss.summary$cp[3], pch = 4, col = "red", lwd = 7)
X = rnorm(100)
eps = rnorm(100)
beta0 = 3
beta1 = 2
beta2 = -3
beta3 = 0.3
Y = beta0 + beta1 * X + beta2 * X^2 + beta3 * X^3 + eps
library(leaps)
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
# Find the model size for best cp, BIC and adjr2
which.min(mod.summary$cp)
which.max(mod.summary$adjr2)
v
which.min(mod.summary$bic)
plot(mod.summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l")
points(8, mod.summary$bic[8], pch = 4, col = "red", lwd = 7)
mod.summary$adjr2
plot(mod.summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(8, mod.summary$adjr2[8], pch = 4, col = "red", lwd = 7)
library(leaps)
set.seed(1)
X = rnorm(100)
eps = rnorm(100)
Y = 3 + 2 * X + -3 * X^2 + 0.3 * X^3 + eps
library(leaps)
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
which.min(mod.summary$cp)
which.min(mod.summary$bic)
which.max(mod.summary$adjr2)
plot(mod.summary$cp, xlab = "Subset Size", ylab = "Cp", pch = 20, type = "l")
points(3, mod.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(mod.summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l")
points(3, mod.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(mod.summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(3, mod.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
coefficients(mod.full)
coefficients(mod.full,id=3)
mod.fwd = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10,
method = "forward")
mod.bwd = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10,
method = "backward")
fwd.summary = summary(mod.fwd)
bwd.summary = summary(mod.bwd)
which.min(fwd.summary$cp)
which.min(fwd.summary$bic)
which.min(fwd.summary$adjr2)
which.min(bwd.summary$cp)
which.min(bwd.summary$bic)
which.min(bwd.summary$adjr2)
which.max(bwd.summary$adjr2)
which.max(fwd.summary$adjr2)
par(mfrow = c(3, 2))
plot(fwd.summary$cp, xlab = "Subset Size", ylab = "Forward Cp", pch = 20, type = "l")
points(3, fwd.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$cp, xlab = "Subset Size", ylab = "Backward Cp", pch = 20, type = "l")
points(3, bwd.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(fwd.summary$bic, xlab = "Subset Size", ylab = "Forward BIC", pch = 20,
type = "l")
points(3, fwd.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$bic, xlab = "Subset Size", ylab = "Backward BIC", pch = 20,
type = "l")
points(3, bwd.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(fwd.summary$adjr2, xlab = "Subset Size", ylab = "Forward Adjusted R2",
pch = 20, type = "l")
points(3, fwd.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
plot(bwd.summary$adjr2, xlab = "Subset Size", ylab = "Backward Adjusted R2",
pch = 20, type = "l")
points(4, bwd.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
par(mfrow = c(3, 2))
plot(mod.summary$cp, xlab = "Subset Size", ylab = "Cp", pch = 20, type = "l")
points(3, mod.summary$cp[3], pch = 4, col = "red", lwd = 7)
plot(mod.summary$bic, xlab = "Subset Size", ylab = "BIC", pch = 20, type = "l")
points(3, mod.summary$bic[3], pch = 4, col = "red", lwd = 7)
plot(mod.summary$adjr2, xlab = "Subset Size", ylab = "Adjusted R2", pch = 20,
type = "l")
points(3, mod.summary$adjr2[3], pch = 4, col = "red", lwd = 7)
library(glmnet)
download.packages(glmnet)
download.packages('glmnet')
install.packages(glmnet)
install.packages("glmnet")
lib(glmnet)
library(glmnet)
coefficients(mod.fwd, id = 3)
coefficients(mod.bwd, id = 3)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
mod.lasso = cv.glmnet(xmat, Y, alpha = 1)
best.lambda = mod.lasso$lambda.min
best.lambda
View(xmat)
xmat1 = model.matrix(y ~ poly(x, 10, raw = F), data = data.full)
View(xmat1)
xmat2 = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)
View(xmat2)
plot(mod.lasso)
plot(mod.lasso)
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
Y = 3 + 5 * X^7 + eps
data.full = data.frame(y = Y, x = X)
mod.full = regsubsets(y ~ poly(x, 10, raw = T), data = data.full, nvmax = 10)
mod.summary = summary(mod.full)
which.min(mod.summary$cp)
which.min(mod.summary$bic)
which.max(mod.summary$adjr2)
coefficients(mod.full, id = 1)
coefficients(mod.full, id = 2)
coefficients(mod.full, id = 4)
xmat = model.matrix(y ~ poly(x, 10, raw = T), data = data.full)[, -1]
mod.lasso = cv.glmnet(xmat, Y, alpha = 1)
best.lambda = mod.lasso$lambda.min
best.model = glmnet(xmat, Y, alpha = 1)
predict(best.model, s = best.lambda, type = "coefficients")
library(ISLR)
?fix()
fix(xmat)
?sum()
?sum
x = College
View(x)
train.size = dim(College)[1] / 2
train = sample(1:dim(College)[1], train.size)
test = -train
College.train = College[train, ]
College.test = College[test, ]
lm.fit = lm(Apps~., data=College.train)
lm.pred = predict(lm.fit, College.test)
mean((College.test[, "Apps"] - lm.pred)^2)
train.mat = model.matrix(Apps~., data=College.train)
test.mat = model.matrix(Apps~., data=College.test)
grid = 10 ^ seq(4, -2, length=100)
mod.ridge = cv.glmnet(train.mat, College.train[, "Apps"], alpha=0, lambda=grid, thresh=1e-12)
lambda.best = mod.ridge$lambda.min
lambda.best
ridge.pred = predict(mod.ridge, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - ridge.pred)^2)
mod.lasso = cv.glmnet(train.mat, College.train[, "Apps"], alpha=1, lambda=grid, thresh=1e-12)
lambda.best = mod.lasso$lambda.min
lambda.best
lasso.pred = predict(mod.lasso, newx=test.mat, s=lambda.best)
mean((College.test[, "Apps"] - lasso.pred)^2)
coefficients(lasso.pred)
coefficients(mod.lasso)
mod.lasso = glmnet(model.matrix(Apps~., data=College), College[, "Apps"], alpha=1)
predict(mod.lasso, s=lambda.best, type="coefficients")
?matrix
/c
?c
p = 20
n = 1000
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
p = 20
n = 1000
x = matrix(rnorm(n * p), n, p)
B = rnorm(p)
B[3] = 0
B[4] = 0
B[9] = 0
B[19] = 0
B[10] = 0
eps = rnorm(p)
y = x %*% B + eps
?rnor
?rnorm
?matrix
?sample
train = sample(seq(1000), 100, replace = FALSE)
y.train = y[train, ]
y.test = y[-train, ]
x.train = x[train, ]
x.test = x[-train, ]
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b")
library(leaps)
regfit.full = regsubsets(y ~ ., data = data.frame(x = x.train, y = y.train),
nvmax = p)
val.errors = rep(NA, p)
x_cols = colnames(x, do.NULL = FALSE, prefix = "x.")
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.train[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.train - pred)^2)
}
plot(val.errors, ylab = "Training MSE", pch = 19, type = "b")
val.errors = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
pred = as.matrix(x.test[, x_cols %in% names(coefi)]) %*% coefi[names(coefi) %in%
x_cols]
val.errors[i] = mean((y.test - pred)^2)
}
plot(val.errors, ylab = "Test MSE", pch = 19, type = "b")
which.min(val.errors)
coef(regfit.full, id = 17)
val.errors = rep(NA, p)
a = rep(NA, p)
b = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
a[i] = length(coefi) - 1
b[i] = sqrt(sum((B[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(B[!(x_cols %in% names(coefi))])^2)
}
plot(x = a, y = b, xlab = "number of coefficients", ylab = "error between estimated and true coefficients")
which.min(b)
val.errors = rep(NA, p)
a = rep(NA, p)
b = rep(NA, p)
for (i in 1:p) {
coefi = coef(regfit.full, id = i)
a[i] = length(coefi) - 1
b[i] = sqrt(sum((B[x_cols %in% names(coefi)] - coefi[names(coefi) %in% x_cols])^2) +
sum(B[!(x_cols %in% names(coefi))])^2)
}
plot(x = a, y = b, xlab = "number of coefficients", ylab = "error between estimated and true coefficients")
which.min(b)
?colnames
colnames(x)
x_cols
install.packages('matlib')
> library(matlib)
> x<-c(0,0.2,0.4,0.6,0.8,1)
> y<-c(0,0,0,1,0,1)
> g<-function(beta){
>   f1<- sum(((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)
>   f2<- sum((((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)*x)
> matrix(c(f1,f2),ncol=1,nrow = 2) >}
> J<-function(beta){
>   f11<-sum((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x))^2)
>   f12<-sum(x*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
> f21<-f12
>   f22<-sum((x^2)*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
> matrix(c(f11,f21,f12,f22),ncol = 2,nrow = 2)
>}
> #starting point for Beta
> beta<-c(-1,1)
> cat("Newton’s method- starting point : beta0=",beta[1],", beta1=",beta[2]) > #Run Newton’s method for 10 iterations
> for(i in 1:10){
> beta<-beta-inv(J(beta))%*%g(beta)
> cat(’\n’,"Newton’s method- iteration" ,i, "beta0=",beta[1],", beta1=",beta[2]) >}
> model_glm<-glm(y~x,family = "binomial")
> cat("By using logistic regression function in R, beta0=",model_glm$coefficients[1],
",   beta1=",model_glm$coefficients[2])
cls
clear
clc
cl
clr
library(matlib)
library(matlib)
library(matlib)
x<-c(0,0.2,0.4,0.6,0.8,1)y<-c(0,0,0,1,0,1)
> g<-function(beta){
>   f1<- sum(((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)
>   f2<- sum((((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)*x)
> matrix(c(f1,f2),ncol=1,nrow = 2) >}
x<-c(0,0.2,0.4,0.6,0.8,1)
y<-c(0,0,0,1,0,1)
g<-function(beta){
f1<- sum(((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)
f2<- sum((((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x)))-y)*x)
matrix(c(f1,f2),ncol=1,nrow = 2)}
J<-function(beta){
f11<-sum((exp(beta[1]+beta[2]*x))/(1+exp(beta[1]+beta[2]*x))^2)
f12<-sum(x*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
f21<-f12
f22<-sum((x^2)*exp(beta[1]+beta[2]*x)/((1+exp(beta[1]+beta[2]*x))^2))
matrix(c(f11,f21,f12,f22),ncol = 2,nrow = 2)
}
beta<-c(-1,1)
for(i in 1:10){
beta<-beta-inv(J(beta))%*%g(beta)
cat(’\n’,"Newton’s method- iteration" ,i, "beta0=",beta[1],", beta1=",beta[2])}
for(i in 1:10){
beta<-beta-inv(J(beta))%*%g(beta)}
detach("package:matlib", unload = TRUE)
library(matlib)
library(updateR)
install.packages('devtools')
library(devtools)
install_github('andreacirilloac/updateR')
library(updateR)
updateR()
library(matlib)
install.packages("matlib")
library(matlib)
remove.packages("rgl")
install.packages("ISLR")
library(ISLR)
attach(OJ)
set.seed(1000)
dim(OJ[1])
train<-sample(1070,800)
library(tree)
oj.tree <- tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)
install.packages("tree")
library(tree)
oj.tree <- tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)
oj.tree <- tree(Purchase ~ ., data = OJ.train)
library(ISLR) attach(OJ)
> set.seed(1000)
>
> train <- sample(dim(OJ)[1], 800)
> OJ.train <- OJ[train, ]
> OJ.test <- OJ[-train, ]
library(ISLR)
attach(OJ)
set.seed(1013)
train = sample(dim(OJ)[1], 800)
OJ.train = OJ[train, ]
OJ.test = OJ[-train, ]
library(tree)
oj.tree = tree(Purchase ~ ., data = OJ.train)
summary(oj.tree)
View(OJ.train)
dim(OJ.train[2])
dim(OJ.train)
dim(OJ.train[0])
dim(OJ.train[1,])
oj.tree
plot(oj,tree)
plot(oj.tree)
text(oj.tree,pretty=0)
oj.pred = predict(oj.tree, OJ.test, type = "class")
table(OJ.test$Purchase, oj.pred)
149+76+45
45/270
cv.oj = cv.tree(oj.tree, FUN = prune.tree)
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "Deviance")
plot(cv.oj$size, cv.oj$dev, type = "b", xlab = "Tree Size", ylab = "CV error")
which.min(cv.oj.dev)
which.min(cv.oj$dev)
cv.oj$dev
cv.oj$size
oj.pruned = prune.tree(oj.tree, best = 6)
summary(oj.pruned)
pred.unpruned = predict(oj.tree, OJ.test, type = "class")
misclass.unpruned = sum(OJ.test$Purchase != pred.unpruned)
misclass.unpruned/length(pred.unpruned)
pred.pruned = predict(oj.pruned, OJ.test, type = "class")
misclass.pruned = sum(OJ.test$Purchase != pred.pruned)
misclass.pruned/length(pred.pruned)
install.packages("learnr")
setwd("~/Desktop/Acads/Stat Learning/Project/Data")
?load
load(dexter_train.data)
pwd
?pwd
load('dexter_train.data')
readRDS('dexter_train.data')
source('dexter_train.data')
read.table('dexter_train.data')
?read.table
read.table('dexter_train.data',sep=':')
read.table(file ='dexter_train.data',sep=':')
plot(cars)
url <- 'https://archive.ics.uci.edu/ml/machine-learning-databases/dexter/DEXTER/dexter_test.data'
input <- scan(url, what = 'character')
data <- as.data.frame(matrix(as.numeric(unlist(strsplit(input, ':'))), ncol = 2))
colnames(data) <- c('Feature','Value')
str(data)
View(data)
View(data)
trial.df <- as.data.frame('dexter_train.data')
View(trial.df)
str(trial.df)
trial.df <- as.data.frame(dexter_train.data)
dexter_train.data
read(dexter_train.data)
obj<- dexter_train.data
